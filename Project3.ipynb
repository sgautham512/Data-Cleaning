{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0lH4xgs8+HL9LzQzkKiPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgautham512/Data-Cleaning/blob/main/Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5LjChL8l_x-p",
        "outputId": "984d745e-3ad9-4b83-88e0-608d21328085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.37.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore<1.38.0,>=1.37.3 (from boto3)\n",
            "  Downloading botocore-1.37.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.3->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.3->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.3->boto3) (1.17.0)\n",
            "Downloading boto3-1.37.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.3-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.3 botocore-1.37.3 jmespath-1.0.1 s3transfer-0.11.3\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "!pip install boto3\n",
        "from botocore.exceptions import ClientError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with your actual AWS credentials\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIATWBJ2J2OGNB3WSMT\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"AN9GTEBkd7usZ2p0GjnNyqTbNbaYP/FCKTiih7xM\""
      ],
      "metadata": {
        "id": "iJa_P0e_JQHM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "# Verify connection by fetching caller identity\n",
        "sts_client = boto3.client('sts')\n",
        "identity = sts_client.get_caller_identity()\n",
        "print(\"Connected to AWS as:\", identity[\"Arn\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiOhylNdJzTV",
        "outputId": "ebbc01f2-96e6-4474-d4ac-acf3e3ec68b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to AWS as: arn:aws:iam::253490777756:user/main_user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "# Let's use Amazon S3\n",
        "s3 = boto3.resource('s3')"
      ],
      "metadata": {
        "id": "vfII19YPAQ63"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out bucket names\n",
        "for bucket in s3.buckets.all():\n",
        "    print(bucket.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjYtj9d0GES8",
        "outputId": "52be605d-5f01-4076-9f68-28c922288415"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my-new-bun-bucket-name-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = \"my-new-bun-bucket-name-1\"\n",
        "\n",
        "#s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': 'eu-central-1'})\n",
        "print(f\"Bucket {bucket_name} created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8OYhlWbJ-zX",
        "outputId": "b5e68b79-1a09-42a0-cd9e-0390b089d0b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket my-new-bun-bucket-name-1 created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('source1.csv', 'rb') as data:\n",
        "    s3.Bucket(bucket_name).put_object(Key='source1.csv', Body=data)"
      ],
      "metadata": {
        "id": "BySoi-sCKobn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('source1.json', 'rb') as data:\n",
        "    s3.Bucket(bucket_name).put_object(Key='source1.json', Body=data)\n",
        "with open('source1.xml', 'rb') as data:\n",
        "    s3.Bucket(bucket_name).put_object(Key='source1.xml', Body=data)"
      ],
      "metadata": {
        "id": "02XpexosNlW3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.client('s3')\n",
        "s3.download_file(bucket_name, 'source1.xml', 'source1.xml')\n",
        "s3.download_file(bucket_name, 'source1.json', 'source1.json')\n",
        "s3.download_file(bucket_name, 'source1.csv', 'source1.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "FE42ubrDP2QA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree\n",
        "import datetime\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Setup logging configuration - this cofiguration not working for some reason\n",
        "logging.basicConfig(\n",
        "    filename='operations.txt',   # Name of the log file\n",
        "    level=logging.INFO,          # Log level (INFO, DEBUG, etc.)\n",
        "    format='%(asctime)s - %(message)s', # Log format with timestamp\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'  # Date and time format\n",
        ")\n",
        "\n",
        "# Create a logger object\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)         # Set the logging level\n",
        "\n",
        "# Create a file handler\n",
        "file_handler = logging.FileHandler('operations.txt')  # Name of the log file\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create a formatter and add it to the handler\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handler to the logger\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "def ExtractDataFromCSV(csv_files):\n",
        "  data_frame = pd.DataFrame()\n",
        "  for file in csv_files:\n",
        "    # Load the CSV into a Pandas DataFrame\n",
        "    # data_frame = pd.read_csv(file)\n",
        "    # print(data_frame)\n",
        "    logger.info(f\"contents of the {file} is appended to list\")\n",
        "    data_frame = pd.concat([data_frame,  pd.read_csv(file)], ignore_index=True)\n",
        "  print(data_frame)\n",
        "  return data_frame\n",
        "\n",
        "def ExtractDataFromJSON(json_files):\n",
        "  data_frame = pd.DataFrame()\n",
        "  for file in json_files:\n",
        "    # Load the json into a Pandas DataFrame\n",
        "    # json_data = pd.read_json(file, lines=True)\n",
        "    # print(json_data)\n",
        "    logger.info(f\"contents of the {file} is appended to list\")\n",
        "    data_frame = pd.concat([data_frame,  pd.read_json(file,lines=True)], ignore_index=True)\n",
        "  print(data_frame)\n",
        "  return data_frame\n",
        "\n",
        "def ExtractDataFromXML(xml_files):\n",
        "  data_frame = pd.DataFrame()\n",
        "  for file in xml_files:\n",
        "    # Load the json into a Pandas DataFrame\n",
        "    # xml_data = pd.read_xml(file)\n",
        "    # print(xml_data)\n",
        "    logger.info(f\"contents of the {file} is appended to list\")\n",
        "    data_frame = pd.concat([data_frame,  pd.read_xml(file)], ignore_index=True)\n",
        "  print(data_frame)\n",
        "  return data_frame\n",
        "\n",
        "def ExtractData(file_path):\n",
        "  main_data_frame = pd.DataFrame()\n",
        "  try:\n",
        "    # csv files are flat structure where things are represented in rows and columns\n",
        "    all_csv_files = glob.glob(f\"{file_path}/*.csv\")\n",
        "    print(all_csv_files)\n",
        "    logger.info(\"Checks for all csv files in the directory\")\n",
        "    logger.info(f\"list of csv files {all_csv_files}\")\n",
        "    main_data_frame = ExtractDataFromCSV(all_csv_files)\n",
        "    print(f\"main function print {main_data_frame}\")\n",
        "\n",
        "    # json structure is heirarchial and not always flat, might require special handling in some cases\n",
        "    # default structure for json [....] bur one can also use the lines are delimiter\n",
        "    # to read such files lines=TRUE should be set\n",
        "    all_json_files = glob.glob(f\"{file_path}/*.json\")\n",
        "    print(all_json_files)\n",
        "    logger.info(\"Checks for all json files in the directory\")\n",
        "    logger.info(f\"list of json files {all_json_files}\")\n",
        "    json_data_frame = ExtractDataFromJSON(all_json_files)\n",
        "    main_data_frame = pd.concat([main_data_frame, json_data_frame], ignore_index=True)\n",
        "\n",
        "    # straight forward method\n",
        "    all_xml_files = glob.glob(f\"{file_path}/*.xml\")\n",
        "    print(all_xml_files)\n",
        "    logger.info(\"Checks for all xml files in the directory\")\n",
        "    logger.info(f\"list of xml files {all_xml_files}\")\n",
        "    xml_data_frame = ExtractDataFromXML(all_xml_files)\n",
        "    main_data_frame = pd.concat([main_data_frame, xml_data_frame], ignore_index=True)\n",
        "\n",
        "     # remove the duplicate entries if there exists something\n",
        "    main_data_frame = main_data_frame.drop_duplicates()\n",
        "    main_data_frame = main_data_frame.reset_index(drop=True)\n",
        "    logger.info(f\"Duplicate entries are removed\")\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    logging.info(f\"Error occurred: {e}\")\n",
        "\n",
        "  return main_data_frame\n",
        "\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "print(f\"current directory - {current_directory}\")\n",
        "df = ExtractData(current_directory)\n",
        "print(df)\n",
        "#convert the entire height column from inches to meters\n",
        "df[\"height\"] = df[\"height\"] * 0.0254\n",
        "logger.info(f\"heights are converted from inches to meter\")\n",
        "\n",
        "#convert the entire weight column from pounds to kgs\n",
        "df[\"weight\"] = df[\"weight\"] * 0.453592\n",
        "logger.info(f\"weights are converted from pounds to kgs\")\n",
        "\n",
        "print(df)\n",
        "\n",
        "df.to_csv('transformed_data.csv', index=False)\n",
        "logger.info(f\"A new csv file created with content from various appended and converted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UvGmD_tdQ4hQ",
        "outputId": "a1b44fdd-711e-4528-ab02-8495ed456498"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Checks for all csv files in the directory\n",
            "INFO:__main__:list of csv files ['/content/source3.csv', '/content/source2.csv', '/content/source1.csv']\n",
            "INFO:__main__:contents of the /content/source3.csv is appended to list\n",
            "INFO:__main__:contents of the /content/source2.csv is appended to list\n",
            "INFO:__main__:contents of the /content/source1.csv is appended to list\n",
            "INFO:__main__:Checks for all json files in the directory\n",
            "INFO:__main__:list of json files ['/content/source1.json', '/content/source3.json', '/content/source2.json']\n",
            "INFO:__main__:contents of the /content/source1.json is appended to list\n",
            "INFO:__main__:contents of the /content/source3.json is appended to list\n",
            "INFO:__main__:contents of the /content/source2.json is appended to list\n",
            "INFO:__main__:Checks for all xml files in the directory\n",
            "INFO:__main__:list of xml files ['/content/source3.xml', '/content/source1.xml', '/content/source2.xml']\n",
            "INFO:__main__:contents of the /content/source3.xml is appended to list\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current directory - /content\n",
            "['/content/source3.csv', '/content/source2.csv', '/content/source1.csv']\n",
            "     name  height  weight\n",
            "0    alex   65.78  112.99\n",
            "1    ajay   71.52  136.49\n",
            "2   alice   69.40  153.03\n",
            "3    ravi   68.22  142.34\n",
            "4     joe   67.79  144.30\n",
            "5    alex   65.78  112.99\n",
            "6    ajay   71.52  136.49\n",
            "7   alice   69.40  153.03\n",
            "8    ravi   68.22  142.34\n",
            "9     joe   67.79  144.30\n",
            "10   alex   65.78  112.99\n",
            "11   ajay   71.52  136.49\n",
            "12  alice   69.40  153.03\n",
            "13   ravi   68.22  142.34\n",
            "14    joe   67.79  144.30\n",
            "main function print      name  height  weight\n",
            "0    alex   65.78  112.99\n",
            "1    ajay   71.52  136.49\n",
            "2   alice   69.40  153.03\n",
            "3    ravi   68.22  142.34\n",
            "4     joe   67.79  144.30\n",
            "5    alex   65.78  112.99\n",
            "6    ajay   71.52  136.49\n",
            "7   alice   69.40  153.03\n",
            "8    ravi   68.22  142.34\n",
            "9     joe   67.79  144.30\n",
            "10   alex   65.78  112.99\n",
            "11   ajay   71.52  136.49\n",
            "12  alice   69.40  153.03\n",
            "13   ravi   68.22  142.34\n",
            "14    joe   67.79  144.30\n",
            "['/content/source1.json', '/content/source3.json', '/content/source2.json']\n",
            "     name  height  weight\n",
            "0    jack   68.70  123.30\n",
            "1     tom   69.80  141.49\n",
            "2   tracy   70.01  136.46\n",
            "3    john   67.90  112.37\n",
            "4    jack   68.70  123.30\n",
            "5     tom   69.80  141.49\n",
            "6   tracy   70.01  136.46\n",
            "7    john   67.90  112.37\n",
            "8    jack   68.70  123.30\n",
            "9     tom   69.80  141.49\n",
            "10  tracy   70.01  136.46\n",
            "11   john   67.90  112.37\n",
            "['/content/source3.xml', '/content/source1.xml', '/content/source2.xml']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:contents of the /content/source1.xml is appended to list\n",
            "INFO:__main__:contents of the /content/source2.xml is appended to list\n",
            "INFO:__main__:Duplicate entries are removed\n",
            "INFO:__main__:heights are converted from inches to meter\n",
            "INFO:__main__:weights are converted from pounds to kgs\n",
            "INFO:__main__:A new csv file created with content from various appended and converted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     name  height  weight\n",
            "0   simon   67.90  112.37\n",
            "1   jacob   66.78  120.67\n",
            "2   cindy   66.49  127.45\n",
            "3    ivan   67.62  114.14\n",
            "4   simon   67.90  112.37\n",
            "5   jacob   66.78  120.67\n",
            "6   cindy   66.49  127.45\n",
            "7    ivan   67.62  114.14\n",
            "8   simon   67.90  112.37\n",
            "9   jacob   66.78  120.67\n",
            "10  cindy   66.49  127.45\n",
            "11   ivan   67.62  114.14\n",
            "     name  height  weight\n",
            "0    alex   65.78  112.99\n",
            "1    ajay   71.52  136.49\n",
            "2   alice   69.40  153.03\n",
            "3    ravi   68.22  142.34\n",
            "4     joe   67.79  144.30\n",
            "5    jack   68.70  123.30\n",
            "6     tom   69.80  141.49\n",
            "7   tracy   70.01  136.46\n",
            "8    john   67.90  112.37\n",
            "9   simon   67.90  112.37\n",
            "10  jacob   66.78  120.67\n",
            "11  cindy   66.49  127.45\n",
            "12   ivan   67.62  114.14\n",
            "     name    height     weight\n",
            "0    alex  1.670812  51.251360\n",
            "1    ajay  1.816608  61.910772\n",
            "2   alice  1.762760  69.413184\n",
            "3    ravi  1.732788  64.564285\n",
            "4     joe  1.721866  65.453326\n",
            "5    jack  1.744980  55.927894\n",
            "6     tom  1.772920  64.178732\n",
            "7   tracy  1.778254  61.897164\n",
            "8    john  1.724660  50.970133\n",
            "9   simon  1.724660  50.970133\n",
            "10  jacob  1.696212  54.734947\n",
            "11  cindy  1.688846  57.810300\n",
            "12   ivan  1.717548  51.772991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "unique_bucket_name = f\"transformBucket-{int(time.time())}\"\n",
        "\n",
        "try:\n",
        "    s3.create_bucket(Bucket=unique_bucket_name)\n",
        "    print(f\"Bucket '{unique_bucket_name}' created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwmBgRUaTU6S",
        "outputId": "89754170-a71f-4310-ff54-00390c3c494c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: An error occurred (InvalidBucketName) when calling the CreateBucket operation: The specified bucket is not valid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "# Let's use Amazon S3\n",
        "s3 = boto3.resource('s3')\n",
        "\n",
        "with open('transformed_data.csv', 'rb') as data:\n",
        "    s3.Bucket(bucket_name).put_object(Key='transformed_data.csv', Body=data)"
      ],
      "metadata": {
        "id": "9ANP0dZmSBl9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mysql-connector-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wbXaeFOBDGR",
        "outputId": "43f8f744-0839-4353-8051-dc773cbe30af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\n",
            "Downloading mysql_connector_python-9.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (34.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.0/34.0 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mysql.connector\n",
        "from mysql.connector import Error"
      ],
      "metadata": {
        "id": "A_rO69MuBJKw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    connection = mysql.connector.connect(\n",
        "    host = \"database-1.cn48wookeoh0.eu-central-1.rds.amazonaws.com\",\n",
        "    port = 3306,\n",
        "    user = \"admin\",\n",
        "    password = \"Sundar_90\",\n",
        "    database=\"ETL\"\n",
        "    )\n",
        "    if connection.is_connected():\n",
        "        print(\"Connection successful!\")\n",
        "        # Ping the server\n",
        "        connection.ping(reconnect=True, attempts=3, delay=5)\n",
        "        print(\"Ping successful!\")\n",
        "        mycursor = connection.cursor()\n",
        "except Error as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJzd9vlJBvst",
        "outputId": "22b4f8c5-ab57-4cdb-95f3-b1cbf744f61c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection successful!\n",
            "Ping successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mycursor.execute(\"show databases\")"
      ],
      "metadata": {
        "id": "IiTbb0FtiDy_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in mycursor:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FAsl5nKiIU2",
        "outputId": "f1b1cce7-9a9d-46b6-ff1b-32e3f45fe69b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ETL',)\n",
            "('information_schema',)\n",
            "('mysql',)\n",
            "('performance_schema',)\n",
            "('sys',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mycursor.execute(\"create database ETL\")"
      ],
      "metadata": {
        "id": "A7tieciiYSOH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_table_and_insert(csv_file_path, table_name, host, user, password, database):\n",
        "    try:\n",
        "        # Load the CSV into a Pandas DataFrame\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "\n",
        "        # Connect to MySQL\n",
        "        connection = mysql.connector.connect(\n",
        "            host=host,\n",
        "            user=user,\n",
        "            password=password,\n",
        "            database=database\n",
        "        )\n",
        "        cursor = connection.cursor()\n",
        "\n",
        "        # Generate SQL table creation statement based on DataFrame\n",
        "        columns = []\n",
        "        for col in df.columns:\n",
        "            # Infer data type\n",
        "            dtype = df[col].dtype\n",
        "            if pd.api.types.is_integer_dtype(dtype):\n",
        "                sql_type = \"INT\"\n",
        "            elif pd.api.types.is_float_dtype(dtype):\n",
        "              sql_type = \"FLOAT\"\n",
        "            elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
        "                sql_type = \"DATETIME\"\n",
        "            else:\n",
        "                max_length = max(df[col].astype(str).apply(len).max(), 255)\n",
        "                sql_type = f\"VARCHAR({max_length})\"\n",
        "            columns.append(f\"`{col}` {sql_type}\")\n",
        "\n",
        "        create_table_query = f\"CREATE TABLE IF NOT EXISTS `{table_name}` ({', '.join(columns)});\"\n",
        "        cursor.execute(create_table_query)\n",
        "        print(f\"Table `{table_name}` created successfully!\")\n",
        "\n",
        "        # Insert data into the table\n",
        "        placeholders = ', '.join(['%s'] * len(df.columns))\n",
        "        insert_query = f\"INSERT INTO `{table_name}` VALUES ({placeholders})\"\n",
        "        cursor.executemany(insert_query, df.values.tolist())\n",
        "        connection.commit()\n",
        "        print(f\"Inserted {cursor.rowcount} rows into `{table_name}`!\")\n",
        "\n",
        "    except Error as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    finally:\n",
        "        if 'connection' in locals() and connection.is_connected():\n",
        "            cursor.close()\n",
        "            connection.close()\n",
        "            print(\"MySQL connection closed.\")"
      ],
      "metadata": {
        "id": "f99RYnQVif2_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "create_table_and_insert(\n",
        "    csv_file_path=\"transformed_data.csv\",       # Path to your CSV file\n",
        "    table_name=\"transformed_data_table\",  # Table name to create\n",
        "    host = \"database-1.cn48wookeoh0.eu-central-1.rds.amazonaws.com\",\n",
        "    user = \"admin\",\n",
        "    password = \"Sundar_90\",\n",
        "    database = \"ETL\"\n",
        " )"
      ],
      "metadata": {
        "id": "YIyiqrKdimDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e00056-fc13-4752-9974-e02fa7a8c26e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table `transformed_data_table` created successfully!\n",
            "Inserted 13 rows into `transformed_data_table`!\n",
            "MySQL connection closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mycursor.execute(\"show tables\")\n",
        "# for x in mycursor:\n",
        "#   print(x)\n",
        "# mycursor.execute(\"describe transformed_data_table\")\n",
        "# print(list(mycursor))\n",
        "\n",
        "mycursor.execute(\"select* from ETL.transformed_data_table\")\n",
        "out = mycursor.fetchall()\n",
        "from tabulate import tabulate\n",
        "print(tabulate(out, headers=[i[0] for i in mycursor.description],showindex=\"always\",tablefmt='psql'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hxGIID3ZjXb",
        "outputId": "65359ce1-cd07-4e79-bebc-72de6d72e8c0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+----------+----------+\n",
            "|    | name   |   height |   weight |\n",
            "|----+--------+----------+----------|\n",
            "|  0 | alex   |  1.67081 |  51.2514 |\n",
            "|  1 | ajay   |  1.81661 |  61.9108 |\n",
            "|  2 | alice  |  1.76276 |  69.4132 |\n",
            "|  3 | ravi   |  1.73279 |  64.5643 |\n",
            "|  4 | joe    |  1.72187 |  65.4533 |\n",
            "|  5 | jack   |  1.74498 |  55.9279 |\n",
            "|  6 | tom    |  1.77292 |  64.1787 |\n",
            "|  7 | tracy  |  1.77825 |  61.8972 |\n",
            "|  8 | john   |  1.72466 |  50.9701 |\n",
            "|  9 | simon  |  1.72466 |  50.9701 |\n",
            "| 10 | jacob  |  1.69621 |  54.7349 |\n",
            "| 11 | cindy  |  1.68885 |  57.8103 |\n",
            "| 12 | ivan   |  1.71755 |  51.773  |\n",
            "+----+--------+----------+----------+\n"
          ]
        }
      ]
    }
  ]
}